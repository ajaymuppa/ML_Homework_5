---

# MachineLearning_HomeAssignment5

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.26%2B-orange.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)

**University of Central Missouri**
**Course:** CS5710 â€” Machine Learning
**Semester:** Fall 2025

---

## ğŸ‘¤ Student Information

* **Name:** AJAY MUPPA
* **Student ID:** 700769264
* **Assignment:** Home Assignment 5

---

## ğŸ“š Table of Contents

1. [Overview](#overview)
2. [Google Colab Notebook](#google-colab-notebook)
3. [Part A â€” Short-Answer Summary](#part-a--short-answer-summary)
4. [Part B â€” Coding Tasks](#part-b--coding-tasks)
5. [Dependencies](#dependencies)
6. [Submission Checklist](#submission-checklist)

---

## ğŸ“Œ Overview

This repository contains my work for **Home Assignment 5**, which includes:

* **Part A:** Written short-answer explanations on core ML/AI topics
* **Part B:** Coding implementations of Transformer components using NumPy and PyTorch

The goal of this assignment is to understand attention, positional encoding, multi-head attention, and ethical considerations in AIâ€”along with practical hands-on coding.

---

## ğŸ”— Google Colab Notebook

Click below to view or execute the assignment notebook:

ğŸ‘‰ **Colab Notebook:**
**[https://colab.research.google.com/drive/1Xh4etPEe4R7S1ODahFcKZ-6Gk66oG_5I?usp=sharing](https://colab.research.google.com/drive/1Xh4etPEe4R7S1ODahFcKZ-6Gk66oG_5I?usp=sharing)**

---

## ğŸ“ Part A â€” Short-Answer Summary

Part A covers detailed theory on:

### ğŸ”¹ Positional Encoding

* Why Transformers need positional information
* Properties of good encoding schemes
* Norm-preserving positional matrices

### ğŸ”¹ Attention Mechanism

* How attention scores are computed
* Role of softmax in normalization
* Context vector formation

### ğŸ”¹ Multi-Head Attention

* Benefits of having multiple attention heads
* Independent subspace projections
* Why concatenation + linear transformation is necessary

### ğŸ”¹ Ethics in AI

* Distinction between ethics, law, and personal feelings
* Utilitarian vs. deontological frameworks
* Why a single ethical standard for AI is difficult

### ğŸ”¹ AI Harms

* Representational vs. allocational harms
* Real-world examples
* Measurement difficulties

### ğŸ”¹ Dataset Bias

* Causes of dataset bias
* Underrepresentation issues
* Bias amplification during training

### ğŸ”¹ Security & Privacy

* Data poisoning attacks
* Model memorization risks
* Model extraction / stealing

---

## ğŸ’» Part B â€” Coding Tasks

### **Q1 â€” Scaled Dot-Product Attention (NumPy)**

Includes:

* Stable softmax
* Scaled dot-product attention
* Optional masking
* Output shape tests

### **Q2 â€” Transformer Encoder Block (PyTorch)**

Includes:

* Manual Multi-Head Self-Attention
* Feed-Forward Network
* Add & Norm layers
* Residual connections
* Dropout
* Input/output shape verification

All scripts are fully commented for clarity and readability.

---

## âš™ï¸ Dependencies

Install required packages:

```bash
pip install numpy torch
```

**Development environment:**

* Python **3.10**
* NumPy **1.26+**
* PyTorch **2.0+**

---

## ğŸ“¤ Submission Checklist

âœ” Fully commented Python code
âœ” Student details included
âœ” README with complete documentation
âœ” Both Part A and Part B included
âœ” Output verification tests added
âœ” Google Colab notebook linked

---
